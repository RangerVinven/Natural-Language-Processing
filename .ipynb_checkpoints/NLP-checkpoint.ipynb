{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18a1ab56-6964-4d1f-b3ba-4f076dcbbb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Documents/Projects/Python/Natural-Language-Processing/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "580715ad-6d20-4dea-ba98-20b52f9a012e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/daniel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/daniel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|â–ˆ| 131306/131306 [00:00<00:00, 2260306.67 examples/\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"Downloading dataset...\")\n",
    "dataset = load_dataset(\"boltuix/emotions-dataset\", split='train')\n",
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "204daad7-7e1b-4a89-b1ed-c37ed38785f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST 5 ROWS:\n",
      "                                            Sentence      Label\n",
      "0  Unfortunately later died from eating tainted m...  happiness\n",
      "1  Last time I saw was loooong ago. Basically bef...    neutral\n",
      "2  You mean by number of military personnel? Beca...    neutral\n",
      "3  Need to go middle of the road no NAME is going...    sadness\n",
      "4           feel melty miserable enough imagine must    sadness\n",
      "\n",
      "DATASET INFORMATION:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 131306 entries, 0 to 131305\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   Sentence  131306 non-null  object\n",
      " 1   Label     131306 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.0+ MB\n",
      "None\n",
      "\n",
      "LABEL DISTRIBUTION:\n",
      "Label\n",
      "happiness    31205\n",
      "sadness      17809\n",
      "neutral      15733\n",
      "anger        13341\n",
      "love         10512\n",
      "fear          8795\n",
      "disgust       8407\n",
      "confusion     8209\n",
      "surprise      4560\n",
      "shame         4248\n",
      "guilt         3470\n",
      "sarcasm       2534\n",
      "desire        2483\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"FIRST 5 ROWS:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDATASET INFORMATION:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nLABEL DISTRIBUTION:\")\n",
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a301abd-1f29-4bc8-8d2b-19f452afc439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/9x6fqw7131x1ct12ncm22bl40000gn/T/ipykernel_2202/3347930940.py:6: DeprecationWarning: 'count' is passed as positional argument\n",
      "  sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence, re.I|re.A)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL FIRST SENTENCE:\n",
      "Unfortunately later died from eating tainted meat NAME BBC documentary dynasties followed the marsh pride the lion episode was awesome\n",
      "\n",
      "CLEANED FIRST SENTENCE:\n",
      "unfortunately later died eating tainted meat name bbc documentary dynasty followed marsh pride lion episode awesome\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence, re.I|re.A)\n",
    "\n",
    "    tokens = sentence.split(\" \")\n",
    "    cleaned_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            cleaned_tokens.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "df[\"Cleaned_Sentence\"] = df[\"Sentence\"].apply(preprocess_sentence)\n",
    "\n",
    "print(\"ORIGINAL FIRST SENTENCE:\")\n",
    "print(df[\"Sentence\"].iloc[0])\n",
    "\n",
    "print(\"\\nCLEANED FIRST SENTENCE:\")\n",
    "print(df[\"Cleaned_Sentence\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58d2563d-221e-4389-ab29-67ee63066b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"Cleaned_Sentence\"]\n",
    "y = df[\"Label\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0, stratify=y)\n",
    "print(\"Total samples: {}\".format(len(x)))\n",
    "print(\"Training samples: {}\".format(len(x_train)))\n",
    "print(\"Testing samples: {}\".format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f764c-d886-4fbc-a239-19ad01f5a1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd6211-5a0f-4f24-a452-005e9b9ebb26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
